{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efa8f725-e730-4d43-8e3f-e661734434ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion & Data Lake Construction\n",
    "\n",
    "## Project: Beauty Lakehouse Analytics Platform\n",
    "\n",
    "In this notebook, we implement the ingestion layer of our data architecture.\n",
    "\n",
    "The objective of this stage is to:\n",
    "\n",
    "- Retrieve raw CSV data from GitHub.\n",
    "- Load the data into Apache Spark DataFrames.\n",
    "- Validate schema and data quality.\n",
    "- Perform initial exploratory analysis.\n",
    "\n",
    "This represents the Data Lake ingestion layer in a modern big data architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "772dde41-1d1d-4bb4-856a-972ecf82ccdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Sources\n",
    "\n",
    "The dataset is retrieved from a public GitHub repository:\n",
    "\n",
    "Repository:\n",
    "https://github.com/zinahalqeji/beauty_lakehouse\n",
    "\n",
    "Raw data location:\n",
    "data/raw/\n",
    "\n",
    "Files used in this project:\n",
    "\n",
    "- customers.csv\n",
    "- products.csv\n",
    "- orders.csv\n",
    "- order_items.csv\n",
    "\n",
    "These datasets represent transactional e-commerce data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2a4c27-896c-44dc-bff2-0a0a58693677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817ba847-97bc-4f1a-a487-5d167fe9b38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/zinahalqeji/beauty_lakehouse/main/data/raw/\"\n",
    "\n",
    "def load_csv_from_github(filename):\n",
    "    url = base_url + filename\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Load using pandas\n",
    "customers_pd = load_csv_from_github(\"customers.csv\")\n",
    "products_pd = load_csv_from_github(\"products.csv\")\n",
    "orders_pd = load_csv_from_github(\"orders.csv\")\n",
    "order_items_pd = load_csv_from_github(\"order_items.csv\")\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "customers_df = spark.createDataFrame(customers_pd)\n",
    "products_df = spark.createDataFrame(products_pd)\n",
    "orders_df = spark.createDataFrame(orders_pd)\n",
    "order_items_df = spark.createDataFrame(order_items_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f871bf77-4141-4079-907e-6dcc15f25b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tables Description\n",
    "\n",
    "### Customers\n",
    "Contains customer information such as customer_id, name, email, country.\n",
    "\n",
    "### Products\n",
    "Contains product catalog information including product_name, category, and price.\n",
    "\n",
    "### Orders\n",
    "Contains order-level information such as order_id and order_date.\n",
    "\n",
    "### Order Items\n",
    "Contains transactional details including product_id and quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110c0451-59af-4803-84b7-0e5bd34f45a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df.printSchema()\n",
    "products_df.printSchema()\n",
    "orders_df.printSchema()\n",
    "order_items_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cec1cba9-fbb4-4b48-ab80-ce79a2cae2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema Validation\n",
    "\n",
    "We validated the schema of each DataFrame to ensure:\n",
    "\n",
    "- Correct column names\n",
    "- Proper data types\n",
    "- No structural inconsistencies\n",
    "\n",
    "Schema validation is critical before performing joins or aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2154ff7d-d756-4dfe-b764-bff6ab3d3cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Customers:\", customers_df.count())\n",
    "print(\"Products:\", products_df.count())\n",
    "print(\"Orders:\", orders_df.count())\n",
    "print(\"Order Items:\", order_items_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2240e200-5bd5-45a8-a5d9-cf1c84a304ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Size Overview\n",
    "\n",
    "After loading the data into Spark:\n",
    "\n",
    "- Customers: 10,000 rows\n",
    "- Products: 2,000 rows\n",
    "- Orders: 100,000 rows\n",
    "- Order Items: 188,573 rows\n",
    "\n",
    "The dataset contains a significant volume of transactional data, making it suitable for distributed processing using Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9864e4d-6cc5-48a9-9556-2e5e4875c8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Quality Check – Missing Values\n",
    "\n",
    "Before proceeding to transformation stages, we validate data quality.\n",
    "\n",
    "We check for NULL values in the Customers dataset to ensure:\n",
    "\n",
    "- Data completeness\n",
    "- Reliable aggregations\n",
    "- Accurate joins\n",
    "\n",
    "Data quality validation is a critical component of big data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d84764-ae1c-4e82-bc61-cee7ad59ddfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "customers_df.select(\n",
    "    [sum(col(c).isNull().cast(\"int\")).alias(c) for c in customers_df.columns]\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "346c3ff3-aa3a-4f5d-9e78-c1e4f0827cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Quality Results\n",
    "\n",
    "The output above shows the number of NULL values for each column.\n",
    "\n",
    "If all values are 0:\n",
    "→ The dataset has no missing values in the Customers table.\n",
    "\n",
    "This step ensures reliability before building the Document Database and Data Warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd55c255-d3e3-4359-9633-cee3a25bd280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Analysis – Orders per Customer\n",
    "\n",
    "We analyze how many orders each customer has placed.\n",
    "\n",
    "This helps identify:\n",
    "- Frequent buyers\n",
    "- Potential high-value customers\n",
    "- Purchasing behavior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c705ee-57c6-4d2e-b932-3abfba882f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df.groupBy(\"customer_id\").count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1677d65f-c244-466b-9944-c35fe80f5e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Interpretation\n",
    "\n",
    "The output shows the number of orders per customer.\n",
    "\n",
    "Customers with higher order counts:\n",
    "→ May represent loyal or high-value clients.\n",
    "\n",
    "This insight will support:\n",
    "- Customer Lifetime Value analysis\n",
    "- Recommendation systems\n",
    "- Graph-based modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6d36f7-2990-4d5a-9233-40fea75a89af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Analysis – Product Distribution by Category\n",
    "\n",
    "We examine how products are distributed across categories.\n",
    "\n",
    "This analysis helps:\n",
    "- Understand product diversity\n",
    "- Identify dominant categories\n",
    "- Prepare for revenue aggregation in the Data Warehouse layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4833a9-6251-4c9c-9d2b-48e8b5675685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_df.groupBy(\"category\").count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd76ba2-7984-41d6-b61d-a6eceee46613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Lake Architecture\n",
    "\n",
    "A Data Lake stores raw data in its original format and supports large-scale distributed processing.\n",
    "\n",
    "In this project:\n",
    "- GitHub → Raw Data Storage\n",
    "- Spark → Processing Layer\n",
    "- Future Layers → Document DB, Warehouse, Graph Analytics\n",
    "\n",
    "This notebook establishes the **foundation of our Lakehouse architecture**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "166c8da8-e57c-4404-87fd-931643018c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Persist Data into Delta Tables (Shared Storage)\n",
    "\n",
    "Since DBFS is disabled in our workspace, we store data in the **Hive Metastore** inside a shared database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43f9ce7-bd17-4697-a3ce-5fed98b3eb64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS dm_project;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70f5b79-b959-4ca5-87f0-62c75611c623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(\"dm_project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af90c57-8e86-4a8c-93d5-d88da04a1643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df.write.mode(\"overwrite\").saveAsTable(\"customers\")\n",
    "products_df.write.mode(\"overwrite\").saveAsTable(\"products\")\n",
    "orders_df.write.mode(\"overwrite\").saveAsTable(\"orders\")\n",
    "order_items_df.write.mode(\"overwrite\").saveAsTable(\"order_items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0227c3-887b-425f-ae4d-ab0e90d7de42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d9f382f-ff22-4d46-9305-935a7b73cc43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Summary\n",
    "\n",
    "We successfully:\n",
    "- Ingested raw CSV data from GitHub\n",
    "- Converted the data into Spark DataFrames\n",
    "- Validated schema and row counts\n",
    "- Performed data quality checks\n",
    "- Conducted exploratory analysis\n",
    "- Stored the data as Delta tables in the shared `dm_project` database\n",
    "\n",
    "This completes the **Data Lake ingestion layer** of our Lakehouse architecture.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5280472110414901,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_dataLake_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
